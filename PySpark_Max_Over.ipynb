{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets assume we have duplicate records in our dataset on particular key. We need to remove duplicate records but at the same time need to consider the maximum value of each column among duplicate values. To develop a logic for this requirement, below are the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/23 11:07:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Max_Over()\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+---------------+\n",
      "|Product_id|Product_name|Price|DiscountPercent|\n",
      "+----------+------------+-----+---------------+\n",
      "|       100|      Mobile| 5000|             10|\n",
      "|       100|      Mobile| 7000|              7|\n",
      "|       200|      Laptop|20000|              4|\n",
      "|       200|      Laptop|25000|              8|\n",
      "|       200|      Laptop|22000|             12|\n",
      "+----------+------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data = (\n",
    "    (100, \"Mobile\", 5000, 10),\n",
    "    (100, \"Mobile\", 7000, 7),\n",
    "    (200, \"Laptop\", 20000, 4),\n",
    "    (200, \"Laptop\", 25000, 8),\n",
    "    (200, \"Laptop\", 22000, 12),\n",
    ")\n",
    "\n",
    "defSchema = StructType(\n",
    "    [\n",
    "        StructField(\"Product_id\", IntegerType(), False),\n",
    "        StructField(\"Product_name\", StringType(), False),\n",
    "        StructField(\"Price\", IntegerType(), False),\n",
    "        StructField(\"DiscountPercent\", IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(data=sample_data, schema=defSchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Over() window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import max, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+---------------+---------+------------------+\n",
      "|Product_id|Product_name|Price|DiscountPercent|max_price|maxDiscountPercent|\n",
      "+----------+------------+-----+---------------+---------+------------------+\n",
      "|       100|      Mobile| 5000|             10|     7000|                10|\n",
      "|       100|      Mobile| 7000|              7|     7000|                10|\n",
      "|       200|      Laptop|20000|              4|    25000|                12|\n",
      "|       200|      Laptop|25000|              8|    25000|                12|\n",
      "|       200|      Laptop|22000|             12|    25000|                12|\n",
      "+----------+------------+-----+---------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"Product_id\")\n",
    "\n",
    "dfMax = df.withColumn(\"max_price\", max(\"Price\").over(windowSpec)).withColumn(\n",
    "    \"maxDiscountPercent\", max(\"DiscountPercent\").over(window=windowSpec)\n",
    ")\n",
    "\n",
    "dfMax.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.window.WindowSpec"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(windowSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Max Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+---------------+\n",
      "|Product_id|Product_name|Price|DiscountPercent|\n",
      "+----------+------------+-----+---------------+\n",
      "|       100|      Mobile| 7000|             10|\n",
      "|       100|      Mobile| 7000|             10|\n",
      "|       200|      Laptop|25000|             12|\n",
      "|       200|      Laptop|25000|             12|\n",
      "|       200|      Laptop|25000|             12|\n",
      "+----------+------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSel = dfMax.select(\n",
    "    col(\"Product_id\"),\n",
    "    col(\"Product_name\"),\n",
    "    col(\"max_price\").alias(\"Price\"),\n",
    "    col(\"maxDiscountPercent\").alias(\"DiscountPercent\"),\n",
    ")\n",
    "dfSel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+---------------+\n",
      "|Product_id|Product_name|Price|DiscountPercent|\n",
      "+----------+------------+-----+---------------+\n",
      "|       100|      Mobile| 7000|             10|\n",
      "|       200|      Laptop|25000|             12|\n",
      "+----------+------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfOut = dfSel.dropDuplicates()\n",
    "dfOut.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+---------------+\n",
      "|Product_id|Product_name|Price|DiscountPercent|\n",
      "+----------+------------+-----+---------------+\n",
      "|       100|      Mobile| 7000|             10|\n",
      "|       200|      Laptop|25000|             12|\n",
      "+----------+------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import max, col\n",
    "\n",
    "# create window partition based on product_id\n",
    "windowSpec = Window.partitionBy(\"Product_id\")\n",
    "\n",
    "# create max_price and maxDiscountPercent columns\n",
    "dfMax = df.withColumn(\"max_price\", max(\"Price\").over(window=windowSpec)).withColumn(\n",
    "    \"maxDiscountPercent\", max(\"DiscountPercent\").over(window=windowSpec)\n",
    ")\n",
    "\n",
    "# select the max value\n",
    "dfSel = dfMax.select(\n",
    "    col(\"Product_id\"),\n",
    "    col(\"Product_name\"),\n",
    "    col(\"max_price\").alias(\"Price\"),\n",
    "    col(\"maxDiscountPercent\").alias(\"DiscountPercent\"),\n",
    ")\n",
    "\n",
    "# drop duplicates\n",
    "dfOut = dfSel.dropDuplicates()\n",
    "dfOut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
