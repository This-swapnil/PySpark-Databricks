{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 23:26:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/22 23:26:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 23:26:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Bucketing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pop-os:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Bucketing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x70a6ed511b80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check if bucketing is Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.sources.bucketing.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create sample data for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| PK|          Attribute|\n",
      "+---+-------------------+\n",
      "|  1| 0.1709497137955568|\n",
      "|  2| 0.8051143958005459|\n",
      "|  3| 0.5775925576589018|\n",
      "|  4| 0.9476047869880925|\n",
      "|  5|    0.2093704977577|\n",
      "|  6|0.36664222617947817|\n",
      "|  7| 0.8078688178371882|\n",
      "|  8| 0.7135143433452461|\n",
      "|  9| 0.7195325566306053|\n",
      "| 10|0.31335292311175456|\n",
      "| 11| 0.8062503712025726|\n",
      "| 12|0.10814914646176654|\n",
      "| 13| 0.3362232980701172|\n",
      "| 14| 0.8133304803837667|\n",
      "| 15|0.47649428738170896|\n",
      "| 16|  0.524728096293865|\n",
      "| 17| 0.9701253460019921|\n",
      "| 18| 0.6232167713919952|\n",
      "| 19| 0.5089687568245219|\n",
      "| 20| 0.5467504094508642|\n",
      "+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1, 10000, 1, 10).select(\n",
    "    col(\"id\").alias(\"PK\"), rand(10).alias(\"Attribute\")\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Non-Bucketing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 23:32:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").saveAsTable(\"nonbucketedTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create bucketed table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"parquet\").bucketBy(10, \"PK\").saveAsTable(\"bucketedTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created Bucked and Non-Bucketed Dataframes for Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.table(\"bucketedTable\")\n",
    "df2 = spark.table(\"bucketedTable\")\n",
    "\n",
    "df3 = spark.table(\"nonbucketedTable\")\n",
    "df4 = spark.table(\"nonbucketedTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Broadcast Join by Default if less than 10 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [PK#92L, Attribute#93, Attribute#99]\n",
      "   +- BroadcastHashJoin [PK#92L], [PK#98L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(PK#92L)\n",
      "      :  +- FileScan parquet spark_catalog.default.nonbucketedtable[PK#92L,Attribute#93] Batched: true, DataFilters: [isnotnull(PK#92L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/nonbuc..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=122]\n",
      "         +- Filter isnotnull(PK#98L)\n",
      "            +- FileScan parquet spark_catalog.default.nonbucketedtable[PK#98L,Attribute#99] Batched: true, DataFilters: [isnotnull(PK#98L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/nonbuc..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df4, \"PK\", \"inner\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Disable Broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"saprk.sql.autoBroadcastJoinThreshold\", -1)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------------+\n",
      "|  PK|          Attribute|          Attribute|\n",
      "+----+-------------------+-------------------+\n",
      "|5000|0.18141810315190554|0.18141810315190554|\n",
      "|5001|  0.626988129685516|  0.626988129685516|\n",
      "|5002|0.35421803636733495|0.35421803636733495|\n",
      "|5003|0.21101605231806198|0.21101605231806198|\n",
      "|5004| 0.8189059402224587| 0.8189059402224587|\n",
      "|5005| 0.6920078838955664| 0.6920078838955664|\n",
      "|5006| 0.7664589254286232| 0.7664589254286232|\n",
      "|5007| 0.5541206475613204| 0.5541206475613204|\n",
      "|5008| 0.3621346554845799| 0.3621346554845799|\n",
      "|5009|0.48110035768197124|0.48110035768197124|\n",
      "|5010|0.18924905548078141|0.18924905548078141|\n",
      "|5011| 0.4865077798603922| 0.4865077798603922|\n",
      "|5012| 0.6609186090133667| 0.6609186090133667|\n",
      "|5013| 0.6393114115238754| 0.6393114115238754|\n",
      "|5014| 0.6378813264876587| 0.6378813264876587|\n",
      "|5015| 0.5207868024664773| 0.5207868024664773|\n",
      "|5016| 0.6128985300949695| 0.6128985300949695|\n",
      "|5017| 0.1449002210923449| 0.1449002210923449|\n",
      "|5018|0.04824925422057691|0.04824925422057691|\n",
      "|5019|0.09983449109688891|0.09983449109688891|\n",
      "+----+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df4, \"PK\", \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-bucketed to Non-bucketed join, Both sides would be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PK#92L, Attribute#93, Attribute#125]\n",
      "+- *(2) BroadcastHashJoin [PK#92L], [PK#124L], Inner, BuildRight, false\n",
      "   :- *(2) Filter isnotnull(PK#92L)\n",
      "   :  +- *(2) ColumnarToRow\n",
      "   :     +- FileScan parquet spark_catalog.default.nonbucketedtable[PK#92L,Attribute#93] Batched: true, DataFilters: [isnotnull(PK#92L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/nonbuc..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=228]\n",
      "      +- *(1) Filter isnotnull(PK#124L)\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.nonbucketedtable[PK#124L,Attribute#125] Batched: true, DataFilters: [isnotnull(PK#124L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/nonbuc..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df4, \"PK\", \"inner\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------------+\n",
      "|  PK|          Attribute|          Attribute|\n",
      "+----+-------------------+-------------------+\n",
      "|5000|0.18141810315190554|0.18141810315190554|\n",
      "|5001|  0.626988129685516|  0.626988129685516|\n",
      "|5002|0.35421803636733495|0.35421803636733495|\n",
      "|5003|0.21101605231806198|0.21101605231806198|\n",
      "|5004| 0.8189059402224587| 0.8189059402224587|\n",
      "|5005| 0.6920078838955664| 0.6920078838955664|\n",
      "|5006| 0.7664589254286232| 0.7664589254286232|\n",
      "|5007| 0.5541206475613204| 0.5541206475613204|\n",
      "|5008| 0.3621346554845799| 0.3621346554845799|\n",
      "|5009|0.48110035768197124|0.48110035768197124|\n",
      "|5010|0.18924905548078141|0.18924905548078141|\n",
      "|5011| 0.4865077798603922| 0.4865077798603922|\n",
      "|5012| 0.6609186090133667| 0.6609186090133667|\n",
      "|5013| 0.6393114115238754| 0.6393114115238754|\n",
      "|5014| 0.6378813264876587| 0.6378813264876587|\n",
      "|5015| 0.5207868024664773| 0.5207868024664773|\n",
      "|5016| 0.6128985300949695| 0.6128985300949695|\n",
      "|5017| 0.1449002210923449| 0.1449002210923449|\n",
      "|5018|0.04824925422057691|0.04824925422057691|\n",
      "|5019|0.09983449109688891|0.09983449109688891|\n",
      "+----+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df4, \"PK\", \"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-bucketed to Non-bucketed join, One side would be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PK#92L, Attribute#93, Attribute#87]\n",
      "+- *(2) BroadcastHashJoin [PK#92L], [PK#86L], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=344]\n",
      "   :  +- *(1) Filter isnotnull(PK#92L)\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet spark_catalog.default.nonbucketedtable[PK#92L,Attribute#93] Batched: true, DataFilters: [isnotnull(PK#92L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/nonbuc..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "   +- *(2) Filter isnotnull(PK#86L)\n",
      "      +- *(2) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.bucketedtable[PK#86L,Attribute#87] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(PK#86L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/bucket..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df1, \"PK\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|  PK|           Attribute|           Attribute|\n",
      "+----+--------------------+--------------------+\n",
      "|5004|  0.8189059402224587|  0.8189059402224587|\n",
      "|5005|  0.6920078838955664|  0.6920078838955664|\n",
      "|5016|  0.6128985300949695|  0.6128985300949695|\n",
      "|5021| 0.11717753054645008| 0.11717753054645008|\n",
      "|5030|  0.9748810778061268|  0.9748810778061268|\n",
      "|5061|  0.7166999415848043|  0.7166999415848043|\n",
      "|5063|  0.6289109154464138|  0.6289109154464138|\n",
      "|5076|  0.7869295678081726|  0.7869295678081726|\n",
      "|5080|  0.5114215285181372|  0.5114215285181372|\n",
      "|5095| 0.09274336770599478| 0.09274336770599478|\n",
      "|5127|  0.9905368964232141|  0.9905368964232141|\n",
      "|5129|  0.5371471706713082|  0.5371471706713082|\n",
      "|5140| 0.09013460432692155| 0.09013460432692155|\n",
      "|5150|  0.6534999902211204|  0.6534999902211204|\n",
      "|5156|  0.9531152736774249|  0.9531152736774249|\n",
      "|5157|   0.659088997485559|   0.659088997485559|\n",
      "|5175|0.010991471549378407|0.010991471549378407|\n",
      "|5187|  0.1946353897563421|  0.1946353897563421|\n",
      "|5215|  0.4484065018837108|  0.4484065018837108|\n",
      "|5216|  0.9866966808814606|  0.9866966808814606|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.join(df1, \"PK\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Non-bucketed to Non-bucketed join, no shuffle at both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PK#2L, Attribute#3, Attribute#87]\n",
      "+- *(2) BroadcastHashJoin [PK#2L], [PK#86L], Inner, BuildLeft, false\n",
      "   :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=462]\n",
      "   :  +- *(1) Project [id#0L AS PK#2L, rand(10) AS Attribute#3]\n",
      "   :     +- *(1) Range (1, 10000, step=1, splits=10)\n",
      "   +- *(2) Filter isnotnull(PK#86L)\n",
      "      +- *(2) ColumnarToRow\n",
      "         +- FileScan parquet spark_catalog.default.bucketedtable[PK#86L,Attribute#87] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(PK#86L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/sda3/iNeuron/Data_Engineering/pySpark/spark-warehouse/bucket..., PartitionFilters: [], PushedFilters: [IsNotNull(PK)], ReadSchema: struct<PK:bigint,Attribute:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, \"PK\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|  PK|           Attribute|           Attribute|\n",
      "+----+--------------------+--------------------+\n",
      "|5004|  0.8189059402224587|  0.8189059402224587|\n",
      "|5005|  0.6920078838955664|  0.6920078838955664|\n",
      "|5016|  0.6128985300949695|  0.6128985300949695|\n",
      "|5021| 0.11717753054645008| 0.11717753054645008|\n",
      "|5030|  0.9748810778061268|  0.9748810778061268|\n",
      "|5061|  0.7166999415848043|  0.7166999415848043|\n",
      "|5063|  0.6289109154464138|  0.6289109154464138|\n",
      "|5076|  0.7869295678081726|  0.7869295678081726|\n",
      "|5080|  0.5114215285181372|  0.5114215285181372|\n",
      "|5095| 0.09274336770599478| 0.09274336770599478|\n",
      "|5127|  0.9905368964232141|  0.9905368964232141|\n",
      "|5129|  0.5371471706713082|  0.5371471706713082|\n",
      "|5140| 0.09013460432692155| 0.09013460432692155|\n",
      "|5150|  0.6534999902211204|  0.6534999902211204|\n",
      "|5156|  0.9531152736774249|  0.9531152736774249|\n",
      "|5157|   0.659088997485559|   0.659088997485559|\n",
      "|5175|0.010991471549378407|0.010991471549378407|\n",
      "|5187|  0.1946353897563421|  0.1946353897563421|\n",
      "|5215|  0.4484065018837108|  0.4484065018837108|\n",
      "|5216|  0.9866966808814606|  0.9866966808814606|\n",
      "+----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df2, \"PK\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
