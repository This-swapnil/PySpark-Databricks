{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/nil/anaconda3/envs/spark/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/nil/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nil/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7fbe97e0-b966-4421-8ead-415c0b629930;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 207ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7fbe97e0-b966-4421-8ead-415c0b629930\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/10ms)\n",
      "24/02/26 20:05:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/26 20:05:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:05:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7b3572677fe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 20:06:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# create sample delta table\n",
    "DeltaTable.createOrReplace(sparkSession=spark).tableName(\"dimEmployee\").addColumn(\n",
    "    \"empId\", \"INT\"\n",
    ").addColumn(\"empName\", \"String\").addColumn(\"SSN\", \"String\").location(\n",
    "    \"/mnt/sda3/iNeuron/Data_Engineering/PySpark/Delta_Lake/delta_lake_08\"\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert data\n",
    "spark.sql(\n",
    "    \"insert into dimEmployee values(100,'Mike','2345671'),(200,'David','5632178'),(300,'Mike','1456782')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|empId|empName|    SSN|\n",
      "+-----+-------+-------+\n",
      "|  200|  David|5632178|\n",
      "|  300|   Mike|1456782|\n",
      "|  100|   Mike|2345671|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view data\n",
    "spark.sql(\"select * from dimEmployee\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate encryption/decryption key\n",
    "from cryptography.fernet import Fernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Fernet.generate_key()\n",
    "f = Fernet(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define UDf to Encrypt Data\n",
    "def encrypt_data(data, key):\n",
    "    f = Fernet(key)\n",
    "    dataDB = bytes(data, \"utf-8\")\n",
    "    encrypted_data = f.encrypt(dataDB)\n",
    "    encrypted_data = str(encrypted_data.decode(\"ascii\"))\n",
    "    return encrypted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define UDF to Decrypt Data\n",
    "def decrypt_data(encrypted_data, key):\n",
    "    f = Fernet(key)\n",
    "    decrypted_data = f.decrypt(encrypted_data.encode()).decode()\n",
    "    return decrypted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register UDF's\n",
    "from pyspark.sql.functions import udf, lit, md5\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "encryption = udf(encrypt_data, StringType())\n",
    "decryption = udf(decrypt_data, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+\n",
      "|empId|empName|SSN    |ssn_encrypted                                                                                       |\n",
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+\n",
      "|200  |David  |5632178|gAAAAABl3KizjZ_OAqCMwTiU3TwLTCGOFHUo3EzdLHm_89K631AbORuIK3bL7GdzyXi-TkAHp3gkXYtm3y0B3mneDUGjzVYMGA==|\n",
      "|300  |Mike   |1456782|gAAAAABl3Ki0DJwQ1d9PdO0J0wLOIpyrmyiypB0jSnnSQOT9ancnk-N-fBAsC5wlbqlmUXcew8PEYabwSH6Cs5C_19V-bXFJ5A==|\n",
      "|100  |Mike   |2345671|gAAAAABl3Ki0zPFq82PYmW8pkvL8A090uXlfCLuRrqPftiuvNS0xeqA-mgKlE6wOgBCgJMIwn5JJGm7XU-2QVUR732vXtIUmaw==|\n",
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Encrypt the data\n",
    "df = spark.table(\"dimEmployee\")\n",
    "encryptedD = df.withColumn(\"ssn_encrypted\", encryption(\"SSN\", lit(key)))\n",
    "encryptedD.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+-------------+\n",
      "|empId|empName|SSN    |ssn_encrypted                                                                                       |ssn_decrypted|\n",
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+-------------+\n",
      "|200  |David  |5632178|gAAAAABl3KkNEZfbLobvk3c0IHJLQb0EZAjuduJ2xjIRwlvjVPJxWNxZP6RWSrIY2eW1H5dTiLBBtkjyji-URQyEfG4gOEinlA==|5632178      |\n",
      "|300  |Mike   |1456782|gAAAAABl3KkN9vaPdHd8mIYvxr_Rd8L4DjbqcPUfRxLl9tbYGMI5Cn004DnkRfRMDiidVGc3XZthHRo1nRdd8DDqRBXaiqLbxw==|1456782      |\n",
      "|100  |Mike   |2345671|gAAAAABl3KkNPP_V1hIuVcCawEqcmifGRzThxH5FhloLYBxQd1cs-qNUAlBKMvYfSUz4RocI3AH6apfE7VJXU6gDu0IiO3sulg==|2345671      |\n",
      "+-----+-------+-------+----------------------------------------------------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decrypt the data\n",
    "dcryptedD = encryptedD.withColumn(\n",
    "    \"ssn_decrypted\", decryption(\"ssn_encrypted\", lit(key))\n",
    ")\n",
    "dcryptedD.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
