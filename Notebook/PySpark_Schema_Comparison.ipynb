{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/26 18:43:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/26 18:43:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Schema Comparison\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create 1st Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+\n",
      "|Id |FirstName|LastName|salary|\n",
      "+---+---------+--------+------+\n",
      "|111|Stephen  |King    |2000  |\n",
      "|222|Philipp  |Larkin  |8000  |\n",
      "|333|John     |Smith   |6000  |\n",
      "+---+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empData1 = [\n",
    "    (111, \"Stephen\", \"King\", 2000),\n",
    "    (222, \"Philipp\", \"Larkin\", 8000),\n",
    "    (333, \"John\", \"Smith\", 6000),\n",
    "]\n",
    "empSchema1 = [\"Id\", \"FirstName\", \"LastName\", \"salary\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=empData1, schema=empSchema1)\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create 2nd dataframe with same schema as 1st dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+\n",
      "|Id |FirstName|LastName|salary|\n",
      "+---+---------+--------+------+\n",
      "|444|Thomas   |Frank   |4000  |\n",
      "|555|Stephen  |Fleming |3000  |\n",
      "|666|William  |Pending |7000  |\n",
      "+---+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empData2 = [\n",
    "    (444, \"Thomas\", \"Frank\", 4000),\n",
    "    (555, \"Stephen\", \"Fleming\", 3000),\n",
    "    (666, \"William\", \"Pending\", 7000),\n",
    "]\n",
    "empSchema2 = [\"Id\", \"FirstName\", \"LastName\", \"salary\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data=empData2, schema=empSchema2)\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create 3rd dataframe with **Different** schema from 1st dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "|Id |Name   |salary|\n",
      "+---+-------+------+\n",
      "|777|David  |4000  |\n",
      "|888|Mike   |3000  |\n",
      "|999|Winston|3000  |\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empData3 = [\n",
    "    (777, \"David\", 4000),\n",
    "    (888, \"Mike\", 3000),\n",
    "    (999, \"Winston\", 3000),\n",
    "]\n",
    "empSchema3 = [\"Id\", \"Name\", \"salary\"]\n",
    "\n",
    "df3 = spark.createDataFrame(data=empData3, schema=empSchema3)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compare schema of 1st and 2nd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Matched\n"
     ]
    }
   ],
   "source": [
    "if df1.schema == df2.schema:\n",
    "    print(\"Schema Matched\")\n",
    "else:\n",
    "    print(\"Schema does not matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compare schema of 1st and 3rd dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema does not matched\n"
     ]
    }
   ],
   "source": [
    "if df1.schema == df3.schema:\n",
    "    print(\"Schema Matched\")\n",
    "else:\n",
    "    print(\"Schema does not matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list of columns missing in third dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FirstName', 'LastName']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(df2.columns) - set(df3.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list of columns missing in second dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name']\n"
     ]
    }
   ],
   "source": [
    "print(list(set(df3.columns) - set(df2.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect all possible columns in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FirstName', 'Name', 'LastName', 'Id', 'salary']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allColumns = df1.columns + df3.columns\n",
    "uniqueColumns = list(set(allColumns))\n",
    "uniqueColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+----+\n",
      "|Id |FirstName|LastName|salary|Name|\n",
      "+---+---------+--------+------+----+\n",
      "|111|Stephen  |King    |2000  |NULL|\n",
      "|222|Philipp  |Larkin  |8000  |NULL|\n",
      "|333|John     |Smith   |6000  |NULL|\n",
      "+---+---------+--------+------+----+\n",
      "\n",
      "+---+-------+------+---------+--------+\n",
      "|Id |Name   |salary|FirstName|LastName|\n",
      "+---+-------+------+---------+--------+\n",
      "|777|David  |4000  |NULL     |NULL    |\n",
      "|888|Mike   |3000  |NULL     |NULL    |\n",
      "|999|Winston|3000  |NULL     |NULL    |\n",
      "+---+-------+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in uniqueColumns:\n",
    "    if col not in df1.columns:\n",
    "        df1 = df1.withColumn(col, lit(None))\n",
    "    if col not in df3.columns:\n",
    "        df3 = df3.withColumn(col, lit(None))\n",
    "\n",
    "df1.show(truncate=False)\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMissingColumns(df1, df2):\n",
    "    allColumns = df1.columns + df3.columns\n",
    "    uniqueColumns = list(set(allColumns))\n",
    "    for col in uniqueColumns:\n",
    "        if col not in df1.columns:\n",
    "            df1 = df1.withColumn(col, lit(None))\n",
    "        if col not in df2.columns:\n",
    "            df2 = df2.withColumn(col, lit(None))\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+----+\n",
      "| Id|FirstName|LastName|salary|Name|\n",
      "+---+---------+--------+------+----+\n",
      "|111|  Stephen|    King|  2000|NULL|\n",
      "|222|  Philipp|  Larkin|  8000|NULL|\n",
      "|333|     John|   Smith|  6000|NULL|\n",
      "+---+---------+--------+------+----+\n",
      "\n",
      "+---+-------+------+---------+--------+\n",
      "| Id|   Name|salary|FirstName|LastName|\n",
      "+---+-------+------+---------+--------+\n",
      "|777|  David|  4000|     NULL|    NULL|\n",
      "|888|   Mike|  3000|     NULL|    NULL|\n",
      "|999|Winston|  3000|     NULL|    NULL|\n",
      "+---+-------+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1, df2 = addMissingColumns(df1, df3)\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
