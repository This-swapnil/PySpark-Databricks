{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Split Array Elements into Separate Columns\"\n",
    ").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create sample Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|key|       value|\n",
      "+---+------------+\n",
      "|ABC|   [1, 2, 3]|\n",
      "|XYZ|[2, NULL, 4]|\n",
      "|KLM|      [8, 7]|\n",
      "|IJK|         [5]|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    sc.parallelize(\n",
    "        [[\"ABC\", [1, 2, 3]], [\"XYZ\", [2, None, 4]], [\"KLM\", [8, 7]], [\"IJK\", [5]]]\n",
    "    ),\n",
    "    [\"key\", \"value\"],\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- split array values into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+--------+\n",
      "|key|value[0]|value[1]|value[2]|\n",
      "+---+--------+--------+--------+\n",
      "|ABC|       1|       2|       3|\n",
      "|XYZ|       2|    NULL|       4|\n",
      "|KLM|       8|       7|    NULL|\n",
      "|IJK|       5|    NULL|    NULL|\n",
      "+---+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"key\", df.value[0], df.value[1], df.value[2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to automate the solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Determine the size of each array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------------+\n",
      "|key|       value|NoOfArrayElements|\n",
      "+---+------------+-----------------+\n",
      "|ABC|   [1, 2, 3]|                3|\n",
      "|XYZ|[2, NULL, 4]|                3|\n",
      "|KLM|      [8, 7]|                2|\n",
      "|IJK|         [5]|                1|\n",
      "+---+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSize = df.select(\"key\", \"value\", size(\"value\").alias(\"NoOfArrayElements\"))\n",
    "dfSize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the maximum size of all arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value = dfSize.agg({\"NoOfArrayElements\": \"max\"}).collect()[0][0]\n",
    "max_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UDF to convert Array Elements into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arraySplitIntoCols(df, maxElements):\n",
    "    for i in range(maxElements):\n",
    "        df = df.withColumn(f\"new_col_{i}\", df.value[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UDF Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------+---------+---------+\n",
      "|key|       value|new_col_0|new_col_1|new_col_2|\n",
      "+---+------------+---------+---------+---------+\n",
      "|ABC|   [1, 2, 3]|        1|        2|        3|\n",
      "|XYZ|[2, NULL, 4]|        2|     NULL|        4|\n",
      "|KLM|      [8, 7]|        8|        7|     NULL|\n",
      "|IJK|         [5]|        5|     NULL|     NULL|\n",
      "+---+------------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfOut = arraySplitIntoCols(df, max_value)\n",
    "dfOut.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
